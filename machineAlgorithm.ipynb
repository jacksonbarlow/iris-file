# Import all necessary libraries and output what version is running
import sys
print('Python: {}'.format(sys.version))
import scipy
print('scipy: {}'.format(scipy.__version__))
import numpy
print('numpy: {}'.format(numpy.__version__))
import matplotlib
print('matplotlib: {}'.format(matplotlib.__version__))
import pandas
print('pandas: {}'.format(pandas.__version__))
import sklearn
print('sklearn: {}'.format(sklearn.__version__))

# Import necessary modules from libraries
from pandas import read_csv
from pandas.plotting import scatter_matrix
from matplotlib import pyplot
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

# Fetch the dataset from github
url = "https://raw.githubusercontent.com/jacksonbarlow/iris-file/master/iris.csv"
names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']
dataset  = read_csv(url, names=names)

# Tells us how many rows/columns are in the dataset
print(dataset.shape)

# Shows the first 20 rows in the dataset
print(dataset.head(20))

# Shows summary of different attributes of the database
print(dataset.describe())

# Checks how many rows there are of each class
print(dataset.groupby('class').size())

# Plots the data shown in the table into boxplots of each different column - gives an idea of the distribution of input values
dataset.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)
pyplot.show()

# Plots the data shown in the table into histograms of each different column - gives an idea of the distributions, two of which seem Gaussian/normal
dataset.hist()
pyplot.show()

# Plots the data show in the table into scatterplots, can help to spot relationships between input variables, closer to y=x, the higher the correlation
scatter_matrix(dataset)
pyplot.show()

# Splits the dataset into two, we will use 80% to train, evaluate and select our models and the other 20% to validate
array = dataset.values
X = array[:,0:4]
y = array[:,4]

# Stores training data and validation data
X_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=0.20, random_state=1)
